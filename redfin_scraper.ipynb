{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cp9RWHeFn4Lh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_redfin(base_url, num_pages=9):\n",
        "    all_data = {\n",
        "        'sqft': [],\n",
        "        'price': [],\n",
        "        'beds': [],\n",
        "        'baths': [],\n",
        "        'address': []\n",
        "    }\n",
        "\n",
        "    Headers = {\n",
        "    'Accept': '*/*',\n",
        "    'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
        "    'Accept-Language': 'en-US,en;q=0.5',\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:132.0) Gecko/20100101 Firefox/132.0'\n",
        "    }\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        if page == 1:\n",
        "            url = base_url\n",
        "        else:\n",
        "            url = f\"{base_url}/page-{page}\"\n",
        "\n",
        "        try:\n",
        "\n",
        "          time.sleep(3)\n",
        "          response = requests.get(url, headers=Headers)\n",
        "          response.raise_for_status()\n",
        "          soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "          prices = soup.find_all('span', attrs='bp-Homecard__Price--value')\n",
        "          beds = soup.find_all('span', attrs='bp-Homecard__Stats--beds text-nowrap')\n",
        "          baths = soup.find_all('span', attrs='bp-Homecard__Stats--baths text-nowrap')\n",
        "          addresses = soup.find_all('div', attrs='bp-Homecard__Address flex align-center color-text-primary font-body-xsmall-compact')\n",
        "          sqft = soup.find_all('span', attrs='bp-Homecard__Stats--sqft text-nowrap')\n",
        "\n",
        "          if not prices:\n",
        "              print(f\"No more listings found on page {page}\")\n",
        "              break\n",
        "\n",
        "          all_data['sqft'].extend([sq.text.strip() for sq in sqft])\n",
        "          all_data['price'].extend([price.text.strip() for price in prices])\n",
        "          all_data['beds'].extend([bed.text.strip() for bed in beds])\n",
        "          all_data['baths'].extend([bath.text.strip() for bath in baths])\n",
        "          all_data['address'].extend([address.text.strip() for address in addresses])\n",
        "\n",
        "          print(f\"Scraped page {page} successfully\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error scraping page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "BsIWljnZpETo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://www.redfin.com/city/17150/UT/Salt-Lake-City'\n",
        "df = scrape_redfin(base_url, num_pages=9)"
      ],
      "metadata": {
        "id": "21PDqwdSpL7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee28e64-f515-443a-e466-4ff9f0712274"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped page 1 successfully\n",
            "Scraped page 2 successfully\n",
            "Scraped page 3 successfully\n",
            "Scraped page 4 successfully\n",
            "Scraped page 5 successfully\n",
            "Scraped page 6 successfully\n",
            "Scraped page 7 successfully\n",
            "Scraped page 8 successfully\n",
            "Scraped page 9 successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('real_estate.csv')"
      ],
      "metadata": {
        "id": "M9NjiQCj5gKq"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}